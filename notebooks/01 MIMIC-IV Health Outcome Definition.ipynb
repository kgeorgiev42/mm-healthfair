{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracts outcomes from MIMIC-IV EHR data and links clinical notes from previous inpatient episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install statannotations missingno\n",
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import stats, special\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import missingno as msno\n",
    "from statannotations.Annotator import Annotator\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load raw MIMIC-IV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper-functions for extracting EHR data\n",
    "def dataframe_from_csv(path, compression='gzip', header=0, index_col=0, chunksize=None):\n",
    "    return pd.read_csv(path, compression=compression, header=header, index_col=index_col, chunksize=None)\n",
    "\n",
    "def read_admissions_table(mimic4_path):\n",
    "    admits = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/admissions.csv.gz'))\n",
    "    admits = admits.reset_index()\n",
    "    admits = admits[['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime',\n",
    "                     'race', 'marital_status', 'insurance', 'admission_location', 'discharge_location']]\n",
    "    admits.admittime = pd.to_datetime(admits.admittime)\n",
    "    admits.dischtime = pd.to_datetime(admits.dischtime)\n",
    "    admits.deathtime = pd.to_datetime(admits.deathtime)\n",
    "    ### Get eligible admissions with ED attendance and complete sensitive data\n",
    "    print(\"Original number of admissions:\", admits.shape[0], admits.subject_id.nunique())\n",
    "    admits = admits[(admits.edregtime.notnull()) & (admits.edouttime.notnull())]\n",
    "    admits = admits[(admits.marital_status.notnull()) & (admits.race.notnull()) & (admits.insurance.notnull())]\n",
    "    print(\"Number of admissions with complete data:\", admits.shape[0], admits.subject_id.nunique())\n",
    "    ### Validate timestamps\n",
    "    admits = admits[(admits.edregtime < admits.admittime)]\n",
    "    admits = admits[(admits.edregtime < admits.dischtime) & (admits.edouttime < admits.dischtime)]\n",
    "    admits = admits[(admits.edouttime > admits.edregtime)&(admits.admittime < admits.dischtime)]\n",
    "    print(\"Number of admissions after timestamp validation:\", admits.shape[0], admits.subject_id.nunique())\n",
    "    ### Set ground-truths for prediction\n",
    "    admits['los_days'] = (admits.dischtime - admits.admittime).dt.total_seconds() / (24 * 60 * 60)\n",
    "    admits['ext_stay_7'] = (admits.los_days > 7).astype(int)\n",
    "    #admits['in_hosp_death'] = np.where((admits.deathtime<admits.dischtime)&(admits.deathtime>admits.admittime), 1, 0).astype(int)\n",
    "    ### Get final admissions as prediction target\n",
    "    #admits = admits.sort_values(by=['subject_id', 'admittime']).drop_duplicates(subset='subject_id', keep='last')\n",
    "    #print(\"Number of valid index admissions:\", admits.shape[0])\n",
    "    return admits\n",
    "\n",
    "def read_patients_table(mimic4_path, adm_data):\n",
    "    pats = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/patients.csv.gz'))\n",
    "    pats = pats.reset_index()\n",
    "    print(\"Original number of patients:\", pats.shape[0], pats.subject_id.nunique())\n",
    "    pats = pats[pats.subject_id.isin(adm_data.subject_id)]\n",
    "    print(\"Number of patients with validated ED attendances:\", pats.shape[0], pats.subject_id.nunique())\n",
    "    pats = pats[['subject_id','gender', 'dod', 'anchor_age', 'anchor_year']]\n",
    "    pats['yob']= pats['anchor_year'] - pats['anchor_age']\n",
    "    #pats.dob = pd.to_datetime(pats.dob)\n",
    "    pats.dod = pd.to_datetime(pats.dod)\n",
    "    pats=pats.drop(columns=['anchor_year'])\n",
    "    pats = pd.merge(pats, adm_data, on='subject_id', how='left')\n",
    "    pats['in_hosp_death'] = np.where((pats.dod<pats.dischtime)&(pats.dod>pats.admittime), 1, 0).astype(int)\n",
    "    pats['non_home_discharge'] = ((~pats.discharge_location.str.contains('HOME|DIED|AGAINST ADVICE', na=True))&(pats.in_hosp_death==0)).astype(int)\n",
    "    return pats\n",
    "\n",
    "def read_notes_table(mimic4_path, adm_data, ext_path=None):\n",
    "    notes = dataframe_from_csv(os.path.join(mimic4_path, 'note/discharge.csv.gz'))\n",
    "    notes = notes.reset_index()\n",
    "    print(\"Original number of notes:\", notes.shape[0], notes.subject_id.nunique())\n",
    "    notes = notes[notes.subject_id.isin(adm_data.subject_id)]\n",
    "    notes['charttime'] = pd.to_datetime(notes.charttime)\n",
    "    print(\"Number of notes with validated ED attendances:\", notes.shape[0], notes.subject_id.nunique())\n",
    "    if ext_path:\n",
    "        notes_prep = pd.read_csv(ext_path, sep=',', low_memory=False)\n",
    "        notes_prep = notes_prep.reset_index()\n",
    "        notes = pd.merge(notes, notes_prep, how='left', on='note_id')\n",
    "        print(\"Number of total matching preprocessed notes:\", notes.input.notnull().sum(), notes.target.notnull().sum())\n",
    "        print(\"Unique patients with matching preprocessed notes:\", notes[notes.input.notnull()].subject_id.nunique(), \n",
    "              notes[notes.target.notnull()].subject_id.nunique())\n",
    "        \n",
    "    adm_notes = pd.merge(adm_data[['subject_id', 'hadm_id', 'admittime']], \n",
    "                         notes[['note_id', 'subject_id', 'hadm_id', 'charttime', 'text', 'input', 'target',\n",
    "                                      'input_tokens', 'target_tokens']], how='left', on=['subject_id', 'hadm_id'])\n",
    "    adm_notes = adm_notes[(adm_notes.target.notnull())]\n",
    "    ### Get last admission for each patient to discard notes from last inpatient episode\n",
    "    adm_last = adm_notes.sort_values(['subject_id', 'admittime']).drop_duplicates(subset=['subject_id'], keep='last')\n",
    "    adm_last = adm_last.rename(columns={'admittime':'last_admittime'})\n",
    "    ### Get list of eligible notes as historical data\n",
    "    adm_lkup = pd.merge(adm_notes, adm_last[['subject_id', 'last_admittime']], on='subject_id', how='left')\n",
    "    adm_lkup = adm_lkup[adm_lkup.admittime < adm_lkup.last_admittime]\n",
    "    adm_notes = adm_notes[adm_notes.hadm_id.isin(adm_lkup.hadm_id)]\n",
    "    print(\"Unique patients with matching preprocessed notes:\", notes[notes.input.notnull()].subject_id.nunique(), \n",
    "              notes[notes.target.notnull()].subject_id.nunique())\n",
    "    print('Min, Mean and Max historical notes per patient:', adm_notes.groupby('subject_id').size().min(),\n",
    "          adm_notes.groupby('subject_id').size().mean(), adm_notes.groupby('subject_id').size().max())\n",
    "    return adm_notes\n",
    "\n",
    "def read_icu_table(mimic4_path, adm_data):\n",
    "    icu = dataframe_from_csv(os.path.join(mimic4_path, 'icu/icustays.csv.gz'))\n",
    "    icu = icu.reset_index()\n",
    "    icu = icu[['subject_id', 'hadm_id', 'intime', 'outtime', 'los']]\n",
    "    print(\"Original number of ICU stays:\", icu.shape[0], icu.subject_id.nunique())\n",
    "    icu = icu[(icu.subject_id.isin(adm_data.subject_id))&(icu.hadm_id.isin(adm_data.hadm_id))]\n",
    "    print(\"Number of ICU stays with validated ED attendances:\", icu.shape[0], icu.subject_id.nunique())\n",
    "    icu.intime = pd.to_datetime(icu.intime)\n",
    "    icu.outtime = pd.to_datetime(icu.outtime)\n",
    "    icu_eps = pd.merge(adm_data, icu, how='left', on=['subject_id', 'hadm_id']).sort_values(['subject_id', 'hadm_id', 'intime']).drop_duplicates(subset=['subject_id', 'hadm_id'], \n",
    "                                                                                                                                                 keep='last')\n",
    "    icu_eps['icu_admission'] = np.where((icu_eps.intime>icu_eps.admittime)&(icu_eps.outtime<icu_eps.dischtime), 1, 0)\n",
    "    return icu_eps.rename(columns={'los':'icu_los_days'})\n",
    "\n",
    "########################## DIAGNOSES ##########################\n",
    "def read_diagnoses_icd_table(mimic4_path):\n",
    "    diag = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/diagnoses_icd.csv.gz'))\n",
    "    diag.reset_index(inplace=True)\n",
    "    return diag\n",
    "\n",
    "\n",
    "def read_d_icd_diagnoses_table(mimic4_path):\n",
    "    d_icd = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/d_icd_diagnoses.csv.gz'))\n",
    "    d_icd.reset_index(inplace=True)\n",
    "    return d_icd[['icd_code', 'long_title']]\n",
    "\n",
    "\n",
    "def read_diagnoses(mimic4_path, adm_data):\n",
    "    \n",
    "    dg_df = read_diagnoses_icd_table(mimic4_path).merge(\n",
    "        read_d_icd_diagnoses_table(mimic4_path), how='inner', left_on=['icd_code'], right_on=['icd_code']\n",
    "    )\n",
    "    print(\"Original number of diagnoses:\", dg_df.shape[0], dg_df.subject_id.nunique())\n",
    "    ### Get last admission for each patient and drop it to get comorbidity history\n",
    "    adm_last = adm_data.sort_values(['subject_id', 'admittime']).drop_duplicates(subset=['subject_id'], keep='last')\n",
    "    adm_last = adm_last.rename(columns={'admittime':'last_admittime'})\n",
    "    ### Get list of eligible hospital episodes as historical data\n",
    "    adm_lkup = adm_data[~adm_data.hadm_id.isin(adm_last.hadm_id)]\n",
    "    adm_lkup = pd.merge(adm_lkup, adm_last[['subject_id', 'last_admittime']], on='subject_id', how='left')\n",
    "    adm_lkup = adm_lkup[adm_lkup.admittime < adm_lkup.last_admittime]\n",
    "    ### Filter diagnoses for lookup episodes\n",
    "    dg_df = dg_df[dg_df.subject_id.isin(adm_lkup.subject_id)]\n",
    "    dg_df = dg_df[dg_df.hadm_id.isin(adm_lkup.hadm_id)]\n",
    "    print(\"Number of previous diagnoses recorded in historical ED metadata:\", dg_df.shape[0], dg_df.subject_id.nunique())\n",
    "    return dg_df\n",
    "\n",
    "\n",
    "def standardize_icd(mapping, df, root=False):\n",
    "    \"\"\"Takes an ICD9 -> ICD10 mapping table and a diagnosis dataframe; adds column with converted ICD10 column\"\"\"\n",
    "\n",
    "    def icd_9to10(icd):\n",
    "        # If root is true, only map an ICD 9 -> 10 according to the ICD9's root (first 3 digits)\n",
    "        if root:\n",
    "            icd = icd[:3]\n",
    "        try:\n",
    "            # Many ICD-9's do not have a 1-to-1 mapping; get first index of mapped codes\n",
    "            return mapping.loc[mapping.diagnosis_code == icd].icd10cm.iloc[0]\n",
    "        except:\n",
    "            print(\"Error on code\", icd)\n",
    "            return np.nan\n",
    "\n",
    "    # Create new column with original codes as default\n",
    "    col_name = 'icd10_convert'\n",
    "    if root: col_name = 'root_' + col_name\n",
    "    df[col_name] = df['icd_code'].values\n",
    "\n",
    "    # Group identical ICD9 codes, then convert all ICD9 codes within a group to ICD10\n",
    "    for code, group in df.loc[df.icd_version == 9].groupby(by='icd_code'):\n",
    "        new_code = icd_9to10(code)\n",
    "        for idx in group.index.values:\n",
    "            # Modify values of original df at the indexes in the groups\n",
    "            df.at[idx, col_name] = new_code\n",
    "\n",
    "########################## LABS ##########################\n",
    "def read_labevents_table(mimic4_path):\n",
    "    labevents = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/labevents.csv.gz'), chunksize=1000)\n",
    "    labevents.reset_index(inplace=True)\n",
    "    return labevents[['subject_id', 'itemid', 'hadm_id', 'charttime', 'storetime', 'value', 'valueuom', 'flag']]\n",
    "\n",
    "\n",
    "def read_d_labitems_table(mimic4_path):\n",
    "    labitems = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/d_labitems.csv.gz'), chunksize=1000)\n",
    "    labitems.reset_index(inplace=True)\n",
    "    return labitems[['itemid', 'label', 'category', 'lonic_code']]\n",
    "\n",
    "\n",
    "def read_labs(mimic4_path):\n",
    "    labevents = read_labevents_table(mimic4_path)\n",
    "    labitems =  read_d_labitems_table(mimic4_path)\n",
    "    return labevents(mimic4_path).merge(\n",
    "        labitems, how='inner', left_on=['itemid'], right_on=['itemid']\n",
    "    )\n",
    "\n",
    "########################## PROCEDURES ##########################\n",
    "def read_procedures_icd_table(mimic4_path):\n",
    "    proc = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/procedures_icd.csv.gz'))\n",
    "    proc.reset_index(inplace=True)\n",
    "    return proc\n",
    "\n",
    "\n",
    "def read_d_icd_procedures_table(mimic4_path):\n",
    "    p_icd = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/d_icd_procedures.csv.gz'))\n",
    "    p_icd.reset_index(inplace=True)\n",
    "    return p_icd[['icd_code', 'long_title']]\n",
    "\n",
    "\n",
    "def read_procedures(mimic4_path):\n",
    "    return read_procedures_icd_table(mimic4_path).merge(\n",
    "        read_d_icd_procedures_table(mimic4_path), how='inner', left_on=['icd_code'], right_on=['icd_code']\n",
    "    )\n",
    "\n",
    "########################## MEDICATIONS ##########################\n",
    "def read_prescriptions_table(mimic4_path):\n",
    "    meds = dataframe_from_csv(os.path.join(mimic4_path, 'hosp/prescriptions.csv.gz'))\n",
    "    meds = meds.reset_index()\n",
    "    return meds[['subject_id', 'hadm_id', 'starttime', 'stoptime', 'ndc', 'gsn', 'drug', 'drug_type']]\n",
    "\n",
    "def get_generic_drugs(mapping, df):\n",
    "    \"\"\"Takes NDC product table and prescriptions dataframe; adds column with NDC table's corresponding generic name\"\"\"\n",
    "\n",
    "    def brand_to_generic(ndc):\n",
    "        # We only want the first 2 sections of the NDC code: xxxx-xxxx-xx\n",
    "        matches = list(re.finditer(r\"-\", ndc))\n",
    "        if len(matches) > 1:\n",
    "            ndc = ndc[:matches[1].start()]\n",
    "        try:\n",
    "            return mapping.loc[mapping.PRODUCTNDC == ndc].NONPROPRIETARYNAME.iloc[0]\n",
    "        except:\n",
    "            print(\"Error: \", ndc)\n",
    "            return np.nan\n",
    "\n",
    "    df['generic_drug_name'] = df['ndc'].apply(brand_to_generic)\n",
    "\n",
    "########################## PREPROCESSING ##########################\n",
    "def read_icd_mapping(map_path):\n",
    "    mapping = pd.read_csv(map_path, header=0, delimiter='\\t', low_memory=False, encoding='iso-8859-1')\n",
    "    mapping.diagnosis_description = mapping.diagnosis_description.apply(str.lower)\n",
    "    return mapping\n",
    "\n",
    "def preproc_icd_module(module, icd_map_path=None, map_code_colname=None, only_icd10=True, cc_dict_path=None) -> pd.DataFrame:\n",
    "    \"\"\"Takes an module dataset with ICD codes and puts it in long_format, optionally mapping ICD-codes by a mapping table path\"\"\"\n",
    "\n",
    "    def standardize_icd(mapping, df, root=False):\n",
    "        \"\"\"Takes an ICD9 -> ICD10 mapping table and a modulenosis dataframe; adds column with converted ICD10 column\"\"\"\n",
    "        \n",
    "        def icd_9to10(icd):\n",
    "            # If root is true, only map an ICD 9 -> 10 according to the ICD9's root (first 3 digits)\n",
    "            if root:\n",
    "                icd = icd[:3]\n",
    "            try:\n",
    "                # Many ICD-9's do not have a 1-to-1 mapping; get first index of mapped codes\n",
    "                return mapping.loc[mapping[map_code_colname] == icd].icd10cm.iloc[0]\n",
    "            except:\n",
    "                #print(\"Error on code\", icd)\n",
    "                return np.nan\n",
    "\n",
    "        # Create new column with original codes as default\n",
    "        col_name = 'icd10_convert'\n",
    "        if root: col_name = 'root_' + col_name\n",
    "        df[col_name] = df['icd_code'].values\n",
    "\n",
    "        # Group identical ICD9 codes, then convert all ICD9 codes within a group to ICD10\n",
    "        for code, group in tqdm(df.loc[df.icd_version == 9].groupby(by='icd_code')):\n",
    "            new_code = icd_9to10(code)\n",
    "            for idx in group.index.values:\n",
    "                # Modify values of original df at the indexes in the groups\n",
    "                df.at[idx, col_name] = new_code\n",
    "\n",
    "        if only_icd10:\n",
    "            # Column for just the roots of the converted ICD10 column\n",
    "            df['root'] = df[col_name].apply(lambda x: x[:3] if type(x) is str else np.nan)\n",
    "\n",
    "    print(module.shape)\n",
    "    print('Unique diagnoses', module['icd_code'].nunique())\n",
    "\n",
    "    # Optional ICD mapping if argument passed\n",
    "    if icd_map_path:\n",
    "        icd_map = read_icd_mapping(icd_map_path)\n",
    "        #print(icd_map)\n",
    "        standardize_icd(icd_map, module, root=True)\n",
    "        module = module[module['root_icd10_convert'].notnull()]\n",
    "        print(\"# unique ICD-9 codes\",module[module['icd_version']==9]['icd_code'].nunique())\n",
    "        print(\"# unique ICD-10 codes\",module[module['icd_version']==10]['icd_code'].nunique())\n",
    "        print(\"# unique ICD-10 codes (After converting ICD-9 to ICD-10)\",module['root_icd10_convert'].nunique())\n",
    "        print(\"# unique ICD-10 codes (After clinical grouping ICD-10 codes)\",module['root'].nunique())\n",
    "        print(\"# Unique patients:  \", module.hadm_id.nunique())\n",
    "\n",
    "    module = module[['subject_id','hadm_id','seq_num','long_title','root_icd10_convert']]\n",
    "    #### Create features for long-term chronic conditions\n",
    "    if cc_dict_path:\n",
    "        with open(cc_dict_path, 'r') as json_dict:\n",
    "            ltc_dict = json.load(json_dict)\n",
    "        ### Initialise long-term condition column\n",
    "        module['ltc_code'] = 'Undefined'\n",
    "        for ltc_code, icd_codes in ltc_dict.items():\n",
    "            module['ltc_code'] = np.where(\n",
    "                module['root_icd10_convert'].str.startswith(tuple(icd_codes)),\n",
    "                ltc_code,\n",
    "                module['ltc_code']\n",
    "            )\n",
    "\n",
    "    return module\n",
    "\n",
    "### Helper util function for physical-mental multimorbidity detection\n",
    "def contains_both_ltc_types(ltc_set):\n",
    "    return all(any(code.startswith(prefix) for code in ltc_set) for prefix in ['physltc_', 'menltc_'])\n",
    "\n",
    "def prepare_unique_patient_features(adm_data, diag_data, ltc_dict_path):\n",
    "    ### Get last ED episode for each patient and drop it to get historical features from the EHR\n",
    "    adm_last = adm_data.sort_values(['subject_id', 'admittime']).drop_duplicates(subset=['subject_id'], keep='last')\n",
    "    ### Comorbidity history\n",
    "    diag_flat = diag_data[diag_data.ltc_code!='Undefined']\n",
    "    print(\"Number of previous diagnoses recorded in historical ED metadata:\", diag_data.shape[0], diag_data.subject_id.nunique())\n",
    "    ### Create list for each row in ltc_code column\n",
    "    diag_flat = diag_flat.groupby(['subject_id'])['ltc_code'].agg(set).reset_index()\n",
    "    ### If dict is populated generate categorical columns for each long-term condition\n",
    "    if ltc_dict_path:\n",
    "        with open(ltc_dict_path, 'r') as json_dict:\n",
    "            ltc_dict = json.load(json_dict)\n",
    "        for ltc_code, _ in ltc_dict.items():\n",
    "            diag_flat[ltc_code] = diag_flat['ltc_code'].apply(lambda x: 1 if ltc_code in x else 0)\n",
    "\n",
    "    ### Create features for multimorbidity\n",
    "    diag_flat['phys_men_multimorbidity'] = diag_flat['ltc_code'].apply(contains_both_ltc_types)\n",
    "    diag_flat['n_unique_conditions'] = diag_flat['ltc_code'].apply(len).astype(np.int8)\n",
    "    diag_flat['is_multimorbid'] = np.where((diag_flat['ltc_code'].str.len()>1), 1, 0)\n",
    "    diag_flat['is_complex_multimorbid'] = np.where((diag_flat['ltc_code'].str.len()>3), 1, 0)\n",
    "    ### Merge with base patient data\n",
    "    adm_last = pd.merge(adm_last, diag_flat, on='subject_id', how='left')\n",
    "    adm_last[diag_flat.drop(['subject_id', 'ltc_code'], axis=1).columns] = adm_last[diag_flat.drop(['subject_id', 'ltc_code'], axis=1).columns].fillna(0).astype(np.int8)\n",
    "    return adm_last\n",
    "\n",
    "def read_ndc_mapping(map_path):\n",
    "    ndc_map = pd.read_csv(map_path, header=0, delimiter=',', low_memory=False, encoding='iso-8859-1')\n",
    "    ndc_map.NONPROPRIETARYNAME = ndc_map.NONPROPRIETARYNAME.fillna(\"\")\n",
    "    ndc_map.NONPROPRIETARYNAME = ndc_map.NONPROPRIETARYNAME.apply(str.lower)\n",
    "    ndc_map.columns = list(map(str.lower, ndc_map.columns))\n",
    "    return ndc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse health outcomes and input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_path = '../../data/MIMIC-IV/config/icd9to10.txt'\n",
    "ndc_path = '../../data/MIMIC-IV/config/NDC_product_table.csv'\n",
    "icd10_map_path = '../../data/MIMIC-IV/config/icd10_codes.json'\n",
    "notes_ext_path = '../../data/MIMIC-IV/mimic-iv-bhc.csv'\n",
    "core_path = '../../data/MIMIC-IV/mimiciv/3.1'\n",
    "core_path_ed = '../../data/MIMIC-IV/mimic-iv-ed/3.1'\n",
    "core_path_txt = '../../data/MIMIC-IV/mimic-iv-note/3.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_data = read_admissions_table(core_path)\n",
    "demo_data = read_patients_table(core_path, adm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data.shape, adm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_data.ext_stay_7.value_counts(normalize=True))\n",
    "print(demo_data.in_hosp_death.value_counts(normalize=True))\n",
    "print(demo_data.non_home_discharge.value_counts(normalize=True))\n",
    "print(demo_data.anchor_age.describe())\n",
    "print(demo_data.gender.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_comp_data = read_icu_table(core_path_ed, demo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data.shape, ed_comp_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process chronic condition history in ED attendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_data = read_diagnoses(core_path, ed_comp_data)\n",
    "diag_ext = preproc_icd_module(diag_data, icd_map_path=icd_path, map_code_colname='diagnosis_code', only_icd10=True, cc_dict_path=icd10_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_ext.subject_id.nunique(), diag_ext.hadm_id.nunique(), ed_comp_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create patient-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features = prepare_unique_patient_features(ed_comp_data, diag_ext, ltc_dict_path=icd10_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_features.iloc[2])\n",
    "print(index_features.iloc[2].ltc_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_features.shape)\n",
    "print('Unique patients:', index_features.subject_id.nunique())\n",
    "print('Age distribution:', index_features.anchor_age.describe())\n",
    "print('Gender distribution:', index_features.gender.value_counts())\n",
    "print('-------------------------------')\n",
    "print('Health outcomes')\n",
    "print(index_features.in_hosp_death.value_counts(normalize=True))\n",
    "print(index_features.ext_stay_7.value_counts(normalize=True))\n",
    "print(index_features.non_home_discharge.value_counts(normalize=True))\n",
    "print(index_features.icu_admission.value_counts(normalize=True))\n",
    "print('-------------------------------')\n",
    "print('Comorbidity history')\n",
    "print(index_features.is_multimorbid.value_counts(normalize=True))\n",
    "print(index_features.is_complex_multimorbid.value_counts(normalize=True))\n",
    "print(index_features.phys_men_multimorbidity.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test linkage with clinical notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_data = read_notes_table(core_path_txt, adm_data, ext_path=notes_ext_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_data[['subject_id', 'hadm_id', 'text', 'input', 'target', 'input_tokens', 'target_tokens']].to_csv('../outputs/linked_data/linked_notes.csv',\n",
    "                                                                                                         index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features.shape, index_features.subject_id.nunique(), index_features.hadm_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features.to_csv('../outputs/linked_data/linked_demographics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print example notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "print('Discharge summary (original)')\n",
    "pp.pprint(notes_data.text.iloc[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-----------------------------------')\n",
    "print('Discharge summary (preprocessed)')\n",
    "pp.pprint(notes_data.input.iloc[14])\n",
    "print('-----------------------------------')\n",
    "print('Brief Hospital Course segment:')\n",
    "pp.pprint(notes_data.target.iloc[14])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
